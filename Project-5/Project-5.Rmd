---
title: "Project 5 - Climate change using time series"
author: "Eliott Van Dieren"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    theme: united
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(readr)
library(DescTools)
library(mgcv) # package for generalized additive models 
library(forecast)
library(nlme)
library(car)
library(dplyr)
library(kableExtra)
```

# Introduction

Climate change has been, for a few decades, an important topic, as this global phenomenon has been affecting the Earth for many years. Scientists around the world try to understand the global warming phenomenon, its causes and try to predict its future trajectory in years to come. In this project, we try to predict global temperature by using time series theory. More precisely, we try to answer two main questions : is there a seasonality in the temperature measures, and what is the form of global warming. To do so, we first start by exploring and transforming the data in [Data exploration], then we try modeling the global trend and its residuals by using Generalized Least Squares and ARMA models in [Modeling the temperature using GLS and ARMA]. Thanks to this model, we can answer the two main questions (see [Answering the two main questions]). Additionally, we can now predict the temperature in the future, which is done in [Prediction up to 2050]. In this section, we also compare the GLS model to another model paradigm, which uses Generalized Additive Models and SARIMA models. Lastly, we conclude by stating the take-home messages and further steps that would be interesting to explore in [Conclusion].

# Data exploration

## Dataset Description

The time series which will be used in this project is the temperature anomalies from January 1880 to December 2022. A temperature anomaly is a departure from a global mean value, which is often a large-scale moving average of the planet's temperature. A positive (negative) anomaly in this time series shows that the temperature was warmer (cooler) than the global mean.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}

temperature <- read.csv("Data/temperature.csv",skip = 1,row.names="Year")
temperature <- temperature[,1:12]
temp <- as.numeric(unlist(t(temperature)))
ts_temp <- ts(temp,start=1880,frequency=12)

par(mfrow = c(1, 2))
plot(ts_temp,
     main="Temperature anomalies since 1880",
     xlab = "Years",ylab="Temperature anomalies")
hist(ts_temp,
     breaks = 20,
     prob = TRUE,
     main="Distribution of temperature anomalies",
     xlab="Temperature anomalies")

```

## Dataset transformations

As seen above, one can identify an increasing trend. For prediction purposes, one might only want to focus on recent years. For this reason, we chose to take a subset of the time series, from $1960$ onwards, as one can argue that there is a changing point in the trend around this year, from fluctuating to a more or less linear growth.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
start_y <- 1960
ts_temp <- window(ts_temp,start=start_y)
par(mfrow = c(1, 2))
plot(ts_temp,
     main="Truncated temperature anomalies",
     xlab = "Years",ylab="Temperature anomalies")
hist(ts_temp,
     breaks = 20,
     prob = TRUE,
     main="Distribution of temperature anomalies",
     xlab="Temperature anomalies")
```

# Modeling the temperature using GLS and ARMA

## Identifying a trend

From the above time-series, one now has to find a model which could fit the increasing trend of the temperature anomalies. In this project, we chose to fit the trend by using a few predictors : a linear, quadratic and exponential term with respect to the time, starting at $1$ from January $1960$ onwards (increments at each month). Furthermore, we also added months as factors to show monthly seasonality for the temperatures.. One can argue that most of the earthâ€™s landmass is in the northern globe, there may be some sort of a seasonal behavior as the global mean would therefore be biased to a northern climate behavior.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
lmData <- data.frame(y = ts_temp,
                     month = as.factor(rep(1:12,length(ts_temp)/12)),
                     t = 1:length(ts_temp))

lmfit <- lm(y~ t + I(t^2) + exp(t/20) + month, data=lmData)

par(mfrow = c(1, 2))

plot(ts_temp,
     main="Temperature anomalies and trend fit",
     xlab="Years",
     ylab="Temperature anomalies")
points(seq(start_y, 2023, length=length(ts_temp)),fitted(lmfit), type="l", col="red")

plot(resid(lmfit), type="l", col="red",
     main = "Residuals from the fitted trend",
     xlab = "Months",
     ylab="Residuals from fitted data")

ts_res <- ts(resid(lmfit),start=start_y,frequency=12)
```

From this model, we can have a first idea whether seasonality and if there is an exponential growth of the temperature anomalies. To do this, we show several t-tests below for each coefficients.

```{r}
summary(lmfit)$coefficients %>% as.data.frame(.) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>%
    kable(digit=3,caption = "Summary of the basic model") %>% kable_styling()
```
As one can see above, we see that coefficients are essentially close to zero, as the values we are trying to plot are usually close to zero, as seen in the histograms above. The months coefficients have non-zero values but are less significant, except for March, June, September ($0.09$) and July ($0.06$) with p-values in parentheses.

Furthermore, one observe that the exponential coefficient is actually negative. This is due to the last elements of the time series, as seen in the fitted plot above. One can see that the trend start decreasing around $2020$. If we check the value of the exponential term in the last month of the time-series, we get a value of $\exp(756/20) = 2.6*10^{16}$, and with the negative coefficient, it gives us a total contribution of $-0.161$, compared to an intercept of $-0.05$, a linear contribution of $0.44$ and quadratic trend of $0.66$. So one can conclude that this exponential part of the model actually dampens the temperature anomalies, and this dampening will increase over time. 

However, one big downside using this linear model is that we assume that the data to be iid in order to use significance tests, which is not the case here. In our case, we can avoid this problem by using Generalized Least Squares which take into account the correlation in the noise: 

$$Y = \textbf{X}\beta + \epsilon \text{ with } \epsilon \sim \mathcal{N}(0,\Sigma)$$
where $Y$ is the the temperature anomaly, $X$ is the data matrix (containing the time, its transformation and dummy variables for every month), and $\epsilon$ is the noise following a normal distribution with a covariance $\Sigma \in \mathbb{R}^{N \times N}$, which makes the noise possibly dependent between the $N$ observations. If we know the covariance matrix $\Sigma$, and if it is invertible, one could define the new problem 
$$\tilde{Y} = \tilde{\textbf{X}}\beta + \tilde{\epsilon} \text{ with } \tilde{\epsilon} \sim \mathcal{N}(0,I_{N})$$
where $\tilde{Y} = \Sigma^{-\frac{1}{2}}Y$, $\tilde{\mathbf{X}} = \Sigma^{-\frac{1}{2}}\mathbf{X}$, $\tilde{\epsilon} = \Sigma^{-\frac{1}{2}}\epsilon$, which enables us to consider this new data as iid. The goal is therefore to find this matrix $\Sigma$. To do so, we will use the time series theory to model the residuals as an ARMA model, and give this information to a Generalized Least Squares model. This will enable us to use the significance tests and answer the two main questions.

## Finding the ARMA model for the residuals

From the initial fitted linear model described in [Identifying a trend], we obtained residuals from which we can fit an ARMA model, hence finding the covariance matrix. To do so, we plot the ACF-PACF plots and try to find relevant orders for the $AR(p)$ and $MA(q)$ parts of the model.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
ts_red <- resid(lmfit)
PlotACF(ts_red,
        main = "Residuals from linear model")

```
From the above plots, we clearly identify $AR(p=2)$, as there is a clear cut-off of the PACF values after the second lag. For $q$, it is a bit more complicated but one could argue that after the second lag, there is a small drop from $0.55$ to less than $0.4$, we will therefore use this parameter $q=2$. No additional seasonality has been found for the lags which are multiples of $12$ (number of months), which means $P,Q=0$. Now, one can look at various sub-models from the $ARMA(p=2,q=2)$, and compare AIC and predictive checking to find the best model.

```{r}
fit_22 <- arima(ts_res, order=c(2,0,2))
fit_21 <- arima(ts_res, order=c(2,0,1))
fit_12 <- arima(ts_res, order=c(1,0,2))

n <- length(ts_res)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-step ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_res[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_res[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_res[train+j], order=c(2,0,1))
  Err[2,j+1] <- sum(ts_res[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_res[train+j], order=c(1,0,2))
  Err[3,j+1] <- sum(ts_res[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
err <- rowMeans(Err)

df_fit <- data.frame(Model = c("ARMA(2,2)","ARMA(2,1)","ARMA(1,2)"),
                     AIC = c(fit_22$aic,fit_21$aic,fit_12$aic),
                     "Predictive error" = c(err[1],err[2],err[3]))
df_fit %>% kable(caption = "Comparison of submodels") %>% kable_styling()
```

From above tests, one finds that the $ARMA(p=2,q=2)$ is better than its submodels, both from AIC and predictive checking. Furthermore, we can check the residuals of the fitted ARMA model, and assert their normality :

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}

par(mfrow = c(1, 2))
acf(fit_22$residuals,
    main="ACF from the ARMA(p=2,q=2) residuals")

pacf(fit_22$residuals,
     main="PACF from the ARMA(p=2,q=2) residuals")
```
```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
qqnorm(fit_22$residuals,main = "Normal Q-Q Plot of the ARMA(p=2,q=2) residuals")
qqline(fit_22$residuals)
```
Except for the upper-tail, the residuals are close to the diagonal on the Q-Q Plot, and no more lags stand out from the ACF-PACF plots. Those mean that we explained well the residuals from the linear model by using the ARMA(2,2) process. Now, we can use this structure as our covariance matrix, as seen in the following section.

## Fitting the GLS model and aswering the two main questions

Now that we have a model for the trend (see [Identifying a trend]) and for its residuals (see [Finding the ARMA model for the residuals]), one can define the GLS model as described above, and is written explicitly here:

$$ \tilde{Y} =  \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 \exp(t) + \beta_{4}M + \epsilon$$
with $\epsilon \sim ARMA(p=2,q=2)$ and $M$ being the month factor ($\beta_4$ is therefore a vector in $\mathbb{R}^{12}$).

# Answering the two main questions

To answer the two questions, we first start by showing the summary of the fitted GLS model :
```{r}
R_struct <- corARMA(form=~t, p=2,q=2) # ARMA(2,0,2) correlation structure 
glsfit <- gls(y~t + I(t^2) + exp(t/20) + month, data=lmData, corr=R_struct, method="ML")
```

```{r}
summary(glsfit)$tTable %>% as.data.frame(.) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>%
    kable(digit=3,caption = "Summary of the GLS model") %>% kable_styling()

Anova(glsfit,type=2) %>%
    kable(digit=3,caption = "ANOVA test II of the GLS model") %>% kable_styling()
```

From the above summary, we observe similar results from the original basic linear model, except for the significance of the exponential coefficient, with a p-value of $0.12$. Thanks to this summary, we can now answer the two main questions :

First, we still observe the negative effect of the exponential term, with a negative coefficient of $-7.53e-18$, which is quite low due to the values of the exponential which grows rapidly with respect to time. Therefore, one can logically refute the exponential positive trend of the temperature anomalies, as the exponential term is used to damper the linear and quadratic part of the model instead of increasing it. The quadratic part is essentially used at the start of $1960$, as one can observe in previous fitted trends above.

Regarding seasonality, we observe in the ANOVA test type 2 that the `month` factor has a strong significance, meaning that there is seasonality regarding the temperature month per month. As noted in the introduction, this could be due to more land in the north hemisphere than in the south.

# Prediction up to $2050$

First, one observes that if we kept the exponential term as is, we would have a negative exponential exploding after a few years, which is realistically very unlikely to happen. We therefore remove the exponential trend from the original model, our  model is therefore :

$$\tilde{Y} =  \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon \quad \text{ with } \epsilon \sim ARMA(p=2,q=2)$$
with coefficients :
```{r}
R_struct <- corARMA(form=~t, p=2,q=2) # ARMA(2,0,2) correlation structure 
sig_glsfit <- gls(y~ t + I(t^2)+ month, data=lmData, corr=R_struct, method="ML")

summary(sig_glsfit)$tTable %>% as.data.frame(.) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>%
    kable(digit=3,caption = "Summary of the GLS model without exponential term") %>% kable_styling()
```

Thanks to our model, we can now try predicting the temperature anomalies up to $2050$:

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
end_y <- 2050
n_years <- end_y-2023

start <- length(ts_temp)+1 # Jan-2023
end <- start+12*n_years - 1 # We want up to 2050 

pred_data <- data.frame(month=as.factor(rep(1:12,n_years)),
                     t = start:end)
preds <- predict(sig_glsfit, newdata = pred_data)

ts_pred <- ts(preds,start=2023,frequency = 12)

# Plotting both
autoplot(ts_temp,main="Prediction of temperature anomalies up to 2050")+autolayer(ts_pred)
```

As seen above, we observe an uptrend with seasonal effect. One can observe that the noise is not really seen in those predictions. This can be due to the large time frame which is used here, and that when you forecast far from the observed value, the prediction of the noise will tend to its mean, which is zero. Therefore, the only trend that we observe after such an amount of time is the fitted trend. By removing the exponential term we do not observe the negative dampening which happened earlier.

## Comparison with GAM trend and SARIMA

In this project, we have focused on GLS models in order to create independence in the data and apply significance tests to answer our two main questions. However, one could only be interested in the prediction and not the significance of seasonality or exponential trends. This is why, instead of working with Generalized Least Square, one can try to use other methods to predict the temperature anomalies.

The method presented here consists of fitting the trend with a Generalized Additive model and then find SARIMA parameters for its residuals. This section is considered additional and only gives the reader another way to forecast temperature anomalies, without assessing the two questions. The methods really resemble the one previously presented and lots of analyses are analogous to previous ones, hence the short description of certain results in this section.

First, we start by defining a Generalized Additive Model for trend fitting. A generalized additive model is a generalized linear model which includes non-linear relationships between variable and the response. The response is given by a sum of smooth functions (usually spline basis functions) of the variables as well as any other GLM-compatible variable. For further details, see [Hastie 1986](https://pdodds.w3.uvm.edu/files/papers/others/1986/hastie1986a.pdf). In our case, we use a GAM model with smoothing function on the time `t`, and we also add the months as factors, to add the monthly seasonality.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
gamfit <- gam(y~s(t)+month, data=lmData)

par(mfrow = c(1, 2))
plot(ts_temp,
     main="Temperature anomalies and trend fit",
     xlab="Years",
     ylab="Temperature anomalies")
points(seq(start_y, 2023, length=length(ts_temp)), fitted(gamfit), type="l", col="red")
plot(resid(gamfit), type="l", col="red",
     main = "Residuals from the fitted trend",
     xlab = "Months",
     ylab="Residuals from fitted data")
```

```{r}
summary(gamfit)$coefficients %>% as.data.frame(.) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>%
    kable(digit=3,caption = "Summary of the basic model") %>% kable_styling()
```

From the residuals, we can apply the same type of analysis than previously done with the linear model residuals in [Identifying a trend]. Hereunder is the ACF-PACF plots from the residuals :

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
ts_gam <- ts(resid(gamfit),start=start_y,frequency=12)
PlotACF(ts_gam,
        main = "Residuals from GAM")

# 10log_10(n) = 28.78522
# s = 12, so we will check for k=1,2 and thats it
```

We first observe that those really resembles the ones studied in the previous residual analysis.From the ACF plot, we observe an exponential decay, which goes below the significance threshold after the order $5-6$, but no real cut-off. From the PACF, we observe a cutoff after lag $2$. One could therefore think of $p=2$ for the AR part of the ARMA model, and no clear value for $q$, even though we see an important decrease (yet no cut-off) at $q=2$.


```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
fit_22 <- arima(ts_gam, order=c(2,0,2))
par(mfrow = c(1, 2))
acf(fit_22$residuals,main="ACF from the ARMA(p=2,q=2) residuals")
pacf(fit_22$residuals,main="ACF from the ARMA(p=2,q=2) residuals")
```
```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
qqnorm(fit_22$residuals,main = "Normal Q-Q Plot of the ARMA(p=2,q=2) residuals")
qqline(fit_22$residuals)
```

As before,  PACF and ACF looks good for white noise. QQ-Plots seem reasonable. Now one can have a look at submodels of the ARMA plot and check whether lower orders for the AR and MA component make sense

```{r}
# Checking from (2,0,2) to submodels

fit_21 <- arima(ts_gam, order=c(2,0,1))
fit_12 <- arima(ts_gam, order=c(1,0,2))

#For model comparison, the model with the lowest AIC score is preferred. The absolute values of the AIC scores do not matter. These scores can be negative or positive.  -> fit_22 seem to be the best
# see screenshot for citation

n <- length(ts_gam)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-step ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_gam[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j], order=c(2,0,1))
  Err[2,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j], order=c(1,0,2))
  Err[3,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
err <- rowMeans(Err)

df_fit <- data.frame(Model = c("ARMA(2,2)","ARMA(2,1)","ARMA(1,2)"),
                     AIC = c(fit_22$aic,fit_21$aic,fit_12$aic),
                     "Predictive error" = c(err[1],err[2],err[3]))
df_fit %>% kable(caption = "Comparison of submodels") %>% kable_styling()
```

As previously observed, we still obtain better results for the ARMA(p=2,q=2) model. We can now have a further look into yearly seasonality and differentiation, by plotting ACF and PACF with yearly lags in red.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}

# Both for AIC and rolling -> fit22 seem nice, it also has normal QQplots and residuals are white noise

# On the ts_gam ts
myacf <- acf(ts_gam, main="ACF with 12k lags in red")
season_lags <- seq(0,10*log(length(ts_gam),10), by=12)
points(myacf$lag[1+season_lags], myacf$acf[1+season_lags], col="red", type="h", lwd=1.5)
mypacf <- pacf(ts_gam, main="PACF with 12k lags in red")
points(mypacf$lag[season_lags], mypacf$acf[season_lags], col="red", type="h", lwd=1.5)

big_P_fit <- arima(ts_gam,order=c(2,0,2),seasonal = c(2,0,0))
small_P_fit <- arima(ts_gam,order=c(2,0,2),seasonal = c(1,0,0))

n <- length(ts_gam)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-month ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_gam[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,2),seasonal = c(2,0,0))
  Err[2,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,2),seasonal = c(1,0,0))
  Err[3,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
err <- rowMeans(Err)

df_fit <- data.frame(Model = c("ARMA(p=2,q=2)","SARIMA(2,0,2)x(2,0,0)","SARIMA(2,0,2)x(1,0,0)"),
                     AIC = c(fit_22$aic,big_P_fit$aic,small_P_fit$aic),
                     "Predictive error" = c(err[1],err[2],err[3]))
df_fit %>% kable(caption = "Comparison with or without P-seasonality") %>% kable_styling()
```

Even though it is not clear from the PACF-ACF plots above, one can try to add the dependence on the lags (months) $12$ and $24$ from the PACF plot, and check whether we obtain better AIC and predictive checking than the basic ARMA(p=2,q=2) model, as it is not very computationally expensive to check. One finds that the SARIMA (2,0,2)x(2,0,0)[12] (which is the one we will keep) yields better results than the SARIMA(2,0,2)x(1,0,0) and ARIMA(2,0,2).

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
resid_f_fit <- resid(big_P_fit)
par(mfrow = c(1, 2))
acf(resid_f_fit,main="ACF from the SARIMA(2,0,2)x(2,0,0) residuals")
pacf(resid_f_fit,main="PACF from the SARIMA(2,0,2)x(2,0,0) residuals")
```

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
# So, one sees that the final SARIMA(2,0,2)x(2,0,0) yields both lowest AIC and predictive residuals
qqnorm(resid_f_fit, main = "Normal Q-Q Plot of the SARIMA(2,0,2)x(2,0,0) residuals")
qqline(resid_f_fit)
```

We here observes that the remaining residuals from the fitted SARIMA models are white noise, as no lag is identified from (P)ACF plots and the Q-Q plots look normal. We can now compare this model to the one computed from calling the `auto.arima` function on the GAM residuals, and compare AIC and predictive error

```{r}
auto <- auto.arima(ts_gam)

n <- length(ts_gam)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-step ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_gam[train+j],order=c(2,0,3),seasonal = c(1,0,0))
  Err[1,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,2),seasonal = c(2,0,0))
  Err[2,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2}
err <- rowMeans(Err)

df_fit <- data.frame(Model = c("Autofit","SARIMA(2,0,2)x(2,0,0)"),
                     AIC = c(auto$aic,big_P_fit$aic),
                     "Predictive error" = c(err[1],err[2]))
df_fit %>% kable(caption = "Comparison Autoarima vs SARIMA(2,0,2)x(2,0,0)") %>% kable_styling()
```

Lastly, calling `auto.arima` on the GAM residuals would give us ARIMA(2,0,3)(1,0,0)[12] with zero mean, which is slightly different from what we have found. However, we get lower AIC and lower predictive error.

Finally, we can try predicting using this model, by adding the trend model to the noise from the fitted SARIMA model.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
preds_gam <- predict(gamfit, newdata = pred_data)

preds_arima <- forecast(big_P_fit,12*n_years,c(0.05,0.95))

preds <- preds_arima

preds$mean <- ts(as.numeric(preds$mean) + as.numeric(preds_gam),start=2023,frequency=12)

preds$lower[,1] <- ts(as.numeric(preds$lower[,1])+ as.numeric(preds_gam),start=2023,frequency=12)
preds$lower[,2] <- ts(as.numeric(preds$lower[,2])+ as.numeric(preds_gam),start=2023,frequency=12)
preds$upper[,1] <- ts(as.numeric(preds$upper[,1])+ as.numeric(preds_gam),start=2023,frequency=12)
preds$upper[,2] <- ts(as.numeric(preds$upper[,2])+ as.numeric(preds_gam),start=2023,frequency=12)

preds$x <- ts_temp

plot(preds,main="Temperature anomalies prediction using GAM + SARIMA model")
```

This is quite interesting, as it is completely different from the GLS plot seen above. First, we observe that the GAM model predicts a slow decrease of temperature anomalies up to $2050$. We also observe the work of the SARIMA model for the noise fitting in the first few years for the prediction, before returning to approximately zero as the years increase (as it should be). This essentially means that depending on the underlying trend model that we choose, we can produce completely different long-term trends, which is appealing.

# Conclusion

In conclusion, the goal of this project was to understand temperature anomalies from the perspective of time series, and try to answer to main questions : Whether there is a seasonality in the temperature anomalies, and whether we can refute the exponential trend of those anomalies. To do so, we first tried to fit a trend on the monthly temperature anomalies from $1960$ onwards. By creating a linear model containing a linear, quadratic and exponential component wrt time, and months factors, one could try to apply statistical tests on the fitted coefficients and conclude from that. However, one has to assume the data to be iid, which is not the case in this project. Therefore, we had to use GLS models and fit an ARMA model on its residuals. Once this was done, we could answer the two main questions using significance testing, which in this case is legal due to the transformation with the new noise covariance matrix. We answered negatively regarding the exponential trend of temperature anomalies and positively for the monthly seasonality.

Additionally, we explored another way to predict the temperature anomalies by using GAM and SARIMA models, with analogous methods than the GLS prediction regarding the models' fitting. We observed different predictions up to $2050$, which is interesting. Further steps would be to apply other methods to tackle this problem such as spectral methods to find ARIMA parameters, as well as linking those temperature anomalies to $CO2$ emissions and maybe try to predict the first using the latter in a bigger time-series model. 