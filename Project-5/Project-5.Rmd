---
title: "Project 5 - Climate change using time series"
author: "Eliott Van Dieren"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    theme: united
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(readr)
library(DescTools)
library(mgcv) # package for generalized additive models 
library(forecast)
library(nlme)
library(car)
library(dplyr)
library(kableExtra)
```

# Introduction

Climate change has been, for a few decades, an important topic, as this global phenomenon has been affecting the Earth for many years. Scientists around the world try to understand the global warming phenomenon, its causes and try to predict its future trajectory in years to come. In this project, we try to predict global temperature by using time series theory. More precisely, we try to answer two main questions : is there a seasonality in the temperature measures, and what is the form of global warming. To do so, we first start by exploring and transforming the data in [Data exploration], then we try modeling the global trend and its residuals by using Generalized Least Squares and ARMA models in [Modeling the temperature using GLS and ARMA]. Thanks to this model, we can answer the two main questions in [Answering the two main questions]. Additionally, we can now predict the temperature in the future, which is done in [Prediction up to 2050]. In this section, we also compare the GLS model to another model paradigm, which uses Generalized Additive Models and SARIMA models. Lastly, we conclude by stating the take-home messages and further steps that would be interesting to explore in [Conclusion].

# Data exploration

## Dataset Description

The time series which will be used in this project is the temperature anomalies from January 1880 to December 2022. A temperature anomaly is a departure from a global mean value. A positive (negative) anomaly in this time series shows that the temperature was warmer (cooler) than the global mean.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}

temperature <- read.csv("Data/temperature.csv",skip = 1,row.names="Year")
temperature <- temperature[,1:12]
temp <- as.numeric(unlist(t(temperature)))
ts_temp <- ts(temp,start=1880,frequency=12)

par(mfrow = c(1, 2))
plot(ts_temp,
     main="Temperature anomalies since 1880",
     xlab = "Years",ylab="Temperature anomalies")
hist(ts_temp,breaks = 20,prob = TRUE)

```

## Dataset transformations

As seen above, one can identify an increasing trend. For prediction purposes, one might only want to focus on recent years. For this reason, we chose to take a subset of the time series, from $1960$ onwards, as one can argue that there is a changing point in the trend around this year, from fluctuating to a more or less linear growth.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
start_y <- 1960
ts_temp <- window(ts_temp,start=start_y)
par(mfrow = c(1, 2))
plot(ts_temp,
     main="Truncated temperature anomalies",
     xlab = "Years",ylab="Temperature anomalies")
hist(ts_temp,breaks = 20,prob = TRUE)
```

# Modeling the temperature using GLS and ARMA

## Identifying a trend

From the above time-series, one now has to find a model which could fit the increasing trend of the temperature anomalies. In this project, we chose to fit the trend by using a few predictors : a linear, quadratic and exponential term with respect to the years, starting at $1$ from 1960 onwards. Furthermore, we also added months as factors to show monthly seasonality for the temperatures. One can argue that most of the earthâ€™s landmass is in the northern globe, there may be some sort of a seasonal behavior as the global mean would therefore be biased to a northen climate behavior.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
lmData <- data.frame(y = ts_temp,
                     month = as.factor(rep(1:12,length(ts_temp)/12)),
                     t = 1:length(ts_temp))

lmfit <- lm(y~ t + I(t^2) + exp(t/20) + month, data=lmData) # try to transform and do non linear least square but too complicated

par(mfrow = c(1, 2))

plot(ts_temp,
     main="Temperature anomalies and trend fit",
     xlab="Years",
     ylab="Temperature anomalies")
points(seq(start_y, 2023, length=length(ts_temp)),fitted(lmfit), type="l", col="red")

plot(resid(lmfit), type="l", col="red",
     main = "Residuals from the fitted trend",
     xlab = "Years",
     ylab="Residuals from fitted data")

ts_res <- ts(resid(lmfit),start=start_y,frequency=12)
```

From this model, we can have a first idea whether seasonality and if there is an exponential growth of the temperature anomalies. To do this, we show several t-tests below for each coefficients.

```{r}
summary(lmfit)$coefficients %>% 
  kable(digit=3,caption = "Summary of the basic model") %>% 
  kable_styling()
```

As one can see above, we see that coefficients are essentially close to zero, as the values we are trying to plot are usually close to zero, as seen in the histograms above. Furthermore, we observe a strong significance of all transformations of the time, but less significance for the months, except for March, June, September ($0.09$) and July ($0.06$) with p-values in parentheses. However, one big drawdown using this linear model is that we assume that, to use the Gauss-Markov theorem, the data to be iid, which is not the case here. Therefore, we can apply a trick by using Generalized Least Squares: 

$$Y = \textbf{X}\beta + \epsilon \text{ with } \epsilon \sim \mathcal{N}(0,\Sigma)$$

where $Y$ is the the temperature anomaly, $X$ is the data matrix (containing the time, its transformation and dummy variables for every month), and $\epsilon$ is the noise following a normal distribution with a covariance $\Sigma \in \mathbb{R}^{N \times N}$, which makes the noise possibly dependant between the $N$ observations. If we know the covariance matrix $\Sigma$, and if it is invertible, one could define the new problem 
$$\tilde{Y} = \tilde{\textbf{X}}\beta + \tilde{\epsilon} \text{ with } \tilde{\epsilon} \sim \mathcal{N}(0,I_{N})$$
where $\tilde{Y} = \Sigma^{-\frac{1}{2}}Y$, $\tilde{\mathbf{X}} = \Sigma^{-\frac{1}{2}}\mathbf{X}$, $\tilde{\epsilon} = \Sigma^{-\frac{1}{2}}\epsilon$, which enables us to consider this new data as iid. The goal is therfore to find this matrix $\Sigma$. To do so, we will use the time series theory to model the residuals as an ARMA model, and give this information to a Generalized Least Squares model.

## Finding the ARMA model for the residuals

From the initial fitted linear model described in [Identifying a trend], we got residuals for which we can try to fit an ARMA model on to find the covariance matrix. To do so, we plot the ACF-PACF plots and try to find relevant orders for the $AR(p)$ and $MA(q)$ parts of the model.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
ts_red <- resid(lmfit)
PlotACF(ts_red)

```
From the above plots, we clearly identify $AR(p=2)$, as there is a clear cut-off of the PACF values after the second lag. For $q$, it is a bit more complicated but one could argue that after the second lag, there is a small drop from $0.55$ to less than $0.4$, we will therefore use this parameter $q=2$. No additional seasonality has been found for the lags which are multiples of $12$ (number of months), wich means $P,Q=0$. Now, one can look at various sub-models from the $ARMA(p=2,q=2)$, and compare AIC and predictive checking to find the best model.

```{r}
fit_22 <- arima(ts_res, order=c(2,0,2))
fit_21 <- arima(ts_res, order=c(2,0,1))
fit_12 <- arima(ts_res, order=c(1,0,2))

n <- length(ts_res)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-step ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_res[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_res[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_res[train+j], order=c(2,0,1))
  Err[2,j+1] <- sum(ts_res[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_res[train+j], order=c(1,0,2))
  Err[3,j+1] <- sum(ts_res[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
err <- rowMeans(Err)

df_fit <- data.frame(Model = c("ARMA(2,2)","ARMA(2,1)","ARMA(1,2)"),
                     AIC = c(fit_22$aic,fit_21$aic,fit_12$aic),
                     "Predictive error" = c(err[1],err[2],err[3]))
df_fit %>% kable(caption = "Comparison of submodels") %>% kable_styling()
```

From above tests, one finds that the $ARMA(p=2,q=2)$ is better than its submodels, both from AIC and predictive checking. Furthermore, we can check the residuals of the fitted ARMA model, and assert their normality :

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}

par(mfrow = c(1, 2))
acf(fit_22$residuals); pacf(fit_22$residuals)
```
```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
qqnorm(fit_22$residuals,main = "Normal Q-Q Plot of the ARMA(p=2,q=2) residuals")
qqline(fit_22$residuals)
```
Except for the upper-tail, the residuals are close to the diagonal on the Q-Q Plot, and no more lags stand out from the ACF-PACF plots. Those mean that we explained well the residuals from the linear model by using the ARMA(2,2) process. Now, we can use this structure as our covariance matrix, as seen in the following section.

## Fitting the GLS model

Now that we have a model for the trend (see [Identifying a trend]) and for its residuals (see [Finding the ARMA model for the residuals]), one can define the GLS model as described above, and is written explicitely here:

$$ \tilde{Y} =  \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 \exp(t) + \epsilon \quad \text{ with } \epsilon \sim ARMA(p=2,q=2) $$
 Hereunder is a summary of the fitted GLS model :
```{r}
R_struct <- corARMA(form=~t, p=2,q=2) # ARMA(2,0,2) correlation structure 
glsfit <- gls(y~t + I(t^2) + exp(t/20) + month, data=lmData, corr=R_struct, method="ML")
```

```{r}
out <- summary(glsfit)
out$tTable %>%
  kable(digit=3,caption = "Summary of the GLS model") %>% 
  kable_styling()
```

From the above summary, one can check that the coefficients for the quadratic and the exponential term are equal to zero, and that the exponential coefficient is not considered significant with a p-value of $0.12$. Thanks to this summary, we can now answer the two main questions, as described in the following section.

# Answering the two main questions

To answer the two questions, as we can now work with iid transformed data, we can freely use significance tests. Hereunder is an ANOVA Type II test to assert variable importance in the trend model:

```{r}
Anova(glsfit, type=2) %>% 
  kable(digit=3,caption = "ANOVA Test-II Table for GLS model") %>% 
  kable_styling()
```

To confirm what we have seen in the summary of the GLS model, this ANOVA test confirms that the exponential variable is not significant for this model. *We can therefore refute the exponential trend* of the temperature anomalies. Furthermore, we observe a strong significance regarding the `month` factor, meaning that there is seasonality regarding the temperature month per month. However, the collected data should be without seasonality as it is supposed to represent the global temperature around the world, but as stated in the introduction, this could be due to more land in the north hemisphere than in the south.

# Prediction up to $2050$

First, we can remove the exponential trend from the original model, as it has been shown to be non-significant, our final model is therefore :

$$\tilde{Y} =  \beta_0 + \beta_1 t + \beta_2 t^2 + \epsilon \quad \text{ with } \epsilon \sim ARMA(p=2,q=2)$$
```{r}
R_struct <- corARMA(form=~t, p=2,q=2) # ARMA(2,0,2) correlation structure 
sig_glsfit <- gls(y~t + I(t^2) + month, data=lmData, corr=R_struct, method="ML")
```

Thanks to our model, we can now try predicting the temperature anomalies up to $2050$:

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
end_y <- 2050
n_years <- end_y-2023
  
start <- length(ts_temp)+1 # Jan-2023
end <- start+12*n_years - 1 # We want up to 2050 

pred_data <- data.frame(month=as.factor(rep(1:12,n_years)),
                     t = start:end)

preds <- predict(sig_glsfit, newdata = pred_data)

ts_pred <- ts(preds,start=2023,frequency = 12)

# Plotting both
autoplot(ts_temp)+autolayer(ts_pred)
```

As seen above, we observe an uptrend with seasonal effect. One can observe that the noise is not really seen in those predictions. This can be due to the large timeframe which is used here, and that when you forecast far from the observed value, the prediction of the noise will tend to its mean, which is zero. Therefore, the only trend that we observe after such an amount of time is the fitted trend.

## Comparison with GAM trend and SARIMA

In this project, we have focused on GLS models in order to use the independance trick for the data and apply significance tests to answer our two main questions. However, one could only be interested in the prediction and not the significance of seasonality or exponential trends. This is why, instead of working with Generalized Least Square, one can try to use other methods to predict the temperature anomalies. 

The method presented here consists of fitting the trend with a Generalized Additive model and then find SARIMA parameters for its residuals. This section is considered additional and only gives the reader another way to forecast temperature anomalies, without assessing the two questions.

First, we start by defining a Generalized Additive Model for trend fitting. A generalized additive model is a generalized linear model which includes non-linear relationships between variable and the response. The response is given by a sum of smooth functions (usually spline basis functions) of the variables as well as any other GLM-compatible variable. For further details, see [Hastie 1986](https://pdodds.w3.uvm.edu/files/papers/others/1986/hastie1986a.pdf). In our case, we use a GAM model with smoothing function on the time `t`, and we also add the months as factors, to add the monthly seasonality.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
gamfit <- gam(y~s(t)+month, data=lmData)

par(mfrow = c(1, 2))
plot(ts_temp)
points(seq(start_y, 2023, length=length(ts_temp)), fitted(gamfit), type="l", col="red")
plot(resid(gamfit), type="l", col="red")
``` 

From the residuals, we can apply the same type of analysis than previously done with the linear model residuals in [Identifying a trend]. Hereunder is the ACF-PACF plots from the residuals :

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
ts_gam <- ts(resid(gamfit),start=start_y,frequency=12)
PlotACF(ts_gam)

# 10log_10(n) = 28.78522
# s = 12, so we will check for k=1,2 and thats it
```

We first observe that those really ressembles the ones studied in the previous residual analysis.From the ACF plot, we observe an exponential decay, which goes below the significance threshold after the order $5-6$, but no real cut-off. From the PACF, we observe a cutoff after lag $2$. One could therefore think of $p=2$ for the AR part of the ARMA model, and no clear value for $q$, even though we see an important decrease (yet no cut-off) at $q=2$.


```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
fit_22 <- arima(ts_gam, order=c(2,0,2))
par(mfrow = c(1, 2))
acf(fit_22$residuals); pacf(fit_22$residuals)
```
```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
qqnorm(fit_22$residuals,main = "Normal Q-Q Plot of the ARMA(p=2,q=2) residuals")
qqline(fit_22$residuals)
```

As before,  PACF and ACF looks good for white noise. QQ-Plots seem reasonable.

Now one can have a look at submodels of the ARMA plot and check whether lower orders for the AR and MA component make sense

```{r}
# Checking from (2,0,2) to submodels

fit_21 <- arima(ts_gam, order=c(2,0,1))
fit_12 <- arima(ts_gam, order=c(1,0,2))

#For model comparison, the model with the lowest AIC score is preferred. The absolute values of the AIC scores do not matter. These scores can be negative or positive.  -> fit_22 seem to be the best
# see screenshot for citation

n <- length(ts_gam)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-step ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_gam[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j], order=c(2,0,1))
  Err[2,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j], order=c(1,0,2))
  Err[3,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
err <- rowMeans(Err)

df_fit <- data.frame(Model = c("ARMA(2,2)","ARMA(2,1)","ARMA(1,2)"),
                     AIC = c(fit_22$aic,fit_21$aic,fit_12$aic),
                     "Predictive error" = c(err[1],err[2],err[3]))
df_fit %>% kable(caption = "Comparison of submodels") %>% kable_styling()
```

Discuss the submodels

```{r}

# Both for AIC and rolling -> fit22 seem nice, it also has normal QQplots and residuals are white noise

# On the ts_gam ts
myacf <- acf(ts_gam, main="ACF with 12k lags in red")
season_lags <- seq(0,10*log(length(ts_gam),10), by=12)
points(myacf$lag[1+season_lags], myacf$acf[1+season_lags], col="red", type="h", lwd=1.5)
mypacf <- pacf(ts_gam)
points(mypacf$lag[season_lags], mypacf$acf[season_lags], col="red", type="h", lwd=1.5)

big_P_fit <- arima(ts_gam,order=c(2,0,2),seasonal = c(2,0,0))
small_P_fit <- arima(ts_gam,order=c(2,0,2),seasonal = c(1,0,0))

n <- length(ts_gam)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-month ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_gam[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,2),seasonal = c(2,0,0))
  Err[2,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,2),seasonal = c(1,0,0))
  Err[3,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
err <- rowMeans(Err)
# 0.03237781 0.03222906 0.03243106

df_fit <- data.frame(Model = c("ARMA(p=2,q=2)","SARIMA(2,0,2)x(2,0,0)","SARIMA(2,0,2)x(1,0,0)"),
                     AIC = c(fit_22$aic,big_P_fit$aic,small_P_fit$aic),
                     "Predictive error" = c(err[1],err[2],err[3]))
df_fit %>% kable(caption = "Comparison with or without P-seasonality") %>% kable_styling()
```
Even though it is not clear from the PACF-ACF plots above, one can try to add the dependance on the lags $12$ and $24$ from the PACF plot, and check whether we obtain better AIC and predictive checking than the basic ARMA(2,2) model, as it is not very expensive to check. One finds that the SARIMA (2,0,2)x(2,0,0)[12] yields better results than the SARIMA(2,0,2)x(1,0,0) and ARIMA(2,0,2), which is the one we will keep. Furthermore, calling `auto.arima` would give us ARIMA(2,0,3)(1,0,0)[12] with zero mean, which is slightly different from what we have found. However, we get lower AIC (-1277.461 vs -1278.698) and lower residual error (0.03222906 vs 0.03252401). We further check the residuals and the QQ-plots for the final SARIMA model

```{r}
resid_f_fit <- resid(final_fit)
par(mfrow = c(1, 2))
acf(resid_f_fit); pacf(resid_f_fit)
```

```{r}
# So, one sees that the final SARIMA(2,0,2)x(2,0,0) yields both lowest AIC and predictive residuals
qqnorm(resid_f_fit)
qqline(resid_f_fit)
```

```{r}
auto <- auto.arima(ts_gam)

c(auto$aic,final_fit$aic)

n <- length(ts_gam)
train <- 1:(n-floor(n/3))
Err <- array(0,c(3, floor(n/3)-2+1 )) # rolling 2-step ahead forecast 
for(j in 0:(floor(n/3)-2)){
  fit <- arima(ts_gam[train+j], order=c(2,0,2))
  Err[1,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,2),seasonal = c(2,0,0))
  Err[2,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
  fit <- arima(ts_gam[train+j],order=c(2,0,3),seasonal = c(1,0,0))
  Err[3,j+1] <- sum(ts_gam[c(1,2) + end(train)[1]+j] - predict(fit, n.ahead=2)$pred)^2
}
rowMeans(Err)
```

# Conclusion