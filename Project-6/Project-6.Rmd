---
title: "Functional Data Analyis for Covid in Europe"
author: "Eliott Van Dieren"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    theme: united
    number_sections: true
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(fda)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyverse)
```

# Introduction

Since early 2020, COVID-19 has had a profound impact on the entire world. Scientists tried to analyse the various death, cases and hospitalization curves to protect the population via different sanitary measures such as lockdowns, social distancing and mask wearing. Those methods were put in place in order to "flatten the curve", and avoid peaks of population in hospitals. 

In this project, we focus on the first wave in European countries, and more precisely on the curve of covid cases across those countries. From those curves, we try to find main trends and common patterns. Practically, this can be done using Functional Data Analysis (FDA), and more precisely PCA where principal components are functions and each curve will be a linear combination of those functions.

Regarding the plan of this project, we first start by a description of the dataset and basic transformations. Second, we explore B-Splines smoothing and prepare our data for the PCA computation, which will be the third section of this report. Lastly, we conclude by reporting the take-home messages and next steps. I want to point out that the majority of the reflection on this project has been made with Candice Baud, another Applied Stats student at EPFL. However, the writing of the report and the coding parts are individual work.

# Data exploration

## Dataset Description

The [dataset](https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv) used in this project is made of worldwide daily data about the covid pandemic. Each row describe more than $50$ covid-related variables for a given day, in a given country. As the dataset contains $67$ columns, we will only describe the ones we will use in this project.

* `continent`:  Continent of the daily observation
* `location`: Country name
* `date`: Date of the measurements
* `new_cases_per_million`: Number of new covid cases per million of citizens at `date` in `location`
* `total_cases_per_million`: Cumulative number of covid cases per million of citizens up to `date` in `location`

```{r}
covid_data <- read_csv("./owid-covid-data.csv")

covid_data <- covid_data %>% filter(continent == 'Europe')

locs <- covid_data %>% group_by(location) %>% summarize(n_na = sum(is.na(total_cases_per_million))/length(total_cases_per_million)) %>% filter(n_na < 0.2) %>% dplyr::select(location) # removing countries with more than 20% NA values

# All countries with less than 5 mil people (because they will have the same weights as ones with 60ish mil people like France for PCA, which does not make sense)
# Also removed Russia and Belarus due to shady reports from the authorities
remove_countries <- c("Albania","Andorra","Bosnia and Herzegovina","Croatia","Cyprus","Estonia","Faeroe Islands","Gibraltar","Guernsey","Iceland","Isle of Man","Jersey","Kosovo","Latvia","Liechtenstein","Lithuania","Luxembourg","Malta","Moldova","Monaco","Montenegro","North Macedonia","San Marino","Slovenia","Vatican","Russia","Belarus")
countries <- locs$location
countries <- countries[!(countries %in% remove_countries)]
n_countries <- length(countries)
data <- covid_data %>% dplyr::select(c('date','location','new_cases_per_million','total_cases_per_million')) %>% filter(location %in% countries)
```
From all European countries, we decided to remove the ones with a population less than five million people. This is justified as we are working with cases per million, and therefore each country will further down have the same weight. However, we think that a country like France with more than 65 million people should not be compared with a country like Luxembourg which only has 640 thousands citizens. We further removed Belarus and Russia due to polemical covid cases reports from the authorities. Lastly, we removed all countries which had more than five percent of missing values in `total_cases_per_million`. With all those removals, we went from $51$ countries to $24$. Hereunder is a plot of the logged cumulative number of covid cases for each country.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
data %>% group_by(location) %>% 
  ggplot(aes(x = date,  y = log(total_cases_per_million),color = location)) + 
  geom_line() +
  ggtitle("Cumulative number of cases per million over time (log scale)") + xlab("Date")+ ylab("Cumul. number of cases per million (log)")+
  theme(legend.text = element_text(size=5), legend.key.height= unit(0.2, 'cm'),
        legend.key.width= unit(0.2, 'cm'), legend.title = element_text(size=7)) +
  guides(color = guide_legend(ncol = 1))
```

## Data transformation

With those $24$ remaining countries, we can now try to find the first wave of covid in each of them. To do so, we first computed for each country, the date when the total number of cases per million exceeded $25$ people (arbitrary but ended with good results). Then, we found when looking at various articles, that the first wave in Europe finished around the start of June $2020$. Therefore, we got for each country, a starting date and an end date (first of June $2020$). Hereunder the new cases per million of people are plotted, for each country from their start date to the first of June 2020.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
list_df <- list()
start_dates <- rep(as.Date("2000-01-01"),n_countries)
end_date <- as.Date("2020-06-01")
par(mfrow = c(1, 4))
for (i in 1:n_countries){
  data_country <- data[data$location == countries[i],]
  start_dates[i] <- data_country$date[which(data_country$total_cases_per_million > 25)[1]]
  dates <- seq.Date(start_dates[i],end_date,by="day")
  data_country <- data_country[data_country$date %in% dates,]
  plot(dates,data_country$new_cases_per_million,main=countries[i],type="l")
  abline(v=start_dates[i],col="red")
  abline(v=end_date,col="blue")
  delta_time <- as.numeric(end_date-start_dates[i])
  data_country$time <- (0:delta_time)/delta_time
  data_country$total_cases_per_million <- log(data_country$total_cases_per_million)
  colnames(data_country)[4] ="log_total_cases_per_million"
  list_df[[i]] <- data_country
}

df <- do.call("rbind",list_df) #combine all country dataframes into a single one

```

As seen above, we check that it matches quite nicely all the first waves of the pandemic, which is what we wanted. However, as each country has a different starting date, the curves plotted above do not have the same length, which is a necessary property to apply PCA. Therefore, we will use in the next section a B-Spline smoothing in order to solve this issue, as well as having nice differentiation properties.

# B-Spline smoothing

In this section, we will use a b-spline basis and fit it to each curve to smooth it and then sample a set of values with the same length. Before using splines, we first had to remap the starting date to time zero and end date to $1$, with a linear mapping, which projects all the lines on the same domain $[0,1]$.

Mathematically, we can describe the B-Spline smoothing following [T. Masak - Functional Data Analysis](https://github.com/TMasak/AppStat/blob/main/Slides/11_functional_data.pdf). For a given country, let $\{y_i\}_{i=1,...,N}$ be noisy values of a function $f$ (which is the covid curve function in our case), evaluated at $\{x_i\}_{i=1,...,N} \in [0,T]$, the days in our case. Furthermore, let $\{\beta_i(x)\}_{i=1,...,q}$ be a set of basis functions chosen such that one can approximate $$y(x) \approx \sum_{i=1}^q\xi_j \beta_j(x).$$ Then, by defining the optimization problem to find the $\xi$'s, as $$\min_\xi \|y-B\xi\|_2^2, \text{ where } B = (\beta_j(x_n)),$$ we get $\hat{\xi} = (B^TB)^\dagger B^Ty$. Now, for a given set of evaluation points $\{z_i\}_{i=1,...,M}$, one gets $(f(z_1),f(z_2),...,f(z_M))^T = \tilde{B}\hat{\xi}$ where $\tilde{B} = (\beta_j(z_i))$.

Practically, we chose b-splines of order six, as we want the speed and acceleration of covid cases to be modeled by b-splines of order three (at least), which are derivatives of the original b-spline. We also set $\{z_i\}_{i=1,...,M}$ as the weekly nodes instead of the original daily nodes as it created too much fluctuations when reaching $t=1$.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
df_matrix <- dplyr::select(df,-c("new_cases_per_million","date")) # new cases not needed anymore

smoothing <- function(df,countries) {
  # Computing the smoothing of each row in df using BSplines
  smoothed_list <- list()
  z <- seq(0,1, length=100)
  for (i in 1:length(countries)) {
    df_country <- df[df$location == countries[i],]
    
    weekly_times <- c(seq(0, 1, by = 7/length(df_country$time)),1)
    B <- bsplineS(df_country$time,norder = 6,weekly_times)
    xi_hat <- ginv(t(B) %*% B) %*% t(B) %*% df_country$log_total_cases_per_million
    
    Bplot <- bsplineS(z,norder=6,weekly_times)
    f_hat <- as.vector(Bplot %*% xi_hat)
    smoothed_list[[i]] <- f_hat
  }
  return(mat <- do.call(rbind, smoothed_list))
}
df_matrix <- smoothing(df_matrix,countries)
colnames(df_matrix) <- seq(0,1, length=100)
matplot(colnames(df_matrix), t(df_matrix), type='l', xlab='time', ylab='Smoothes cumulative covid cases', col=1:24,main="Smoothed log-cumulative cases per country")
legend('bottomright',legend=countries,col=1:24,fill = 1:24, ncol = 4,cex = 0.75)
```

Now that we have those smoothed functions, with the same number of evaluations, we can start applying functional PCA and interpret the principal components.

# Functional PCA

In this section, we apply Functional PCA on the $24 \times 100$ matrix previously computed, and extract its principal components.

Mathematically, let a given random sample $\{X_i\}_{i=1,...,N}$ with $X_i \in \mathbb{R}^p$ such that $\mathbb{E}[X_1^2]$.  We define $\hat{\mu} = \frac{1}{N} \sum_{i=1}^N X_i \in \mathbb{R}^p$ and $\hat{C} = \frac{1}{N} \sum_{i=1}^N (X_i-\hat{\mu})(X_i-\hat{\mu})^T$. By eigendecomposition, we get $\hat{C} = \sum_{i=j}^{p \wedge N} \hat{\lambda}_j\hat{e}_j\hat{e}_j^T$. Now, for each sample, define $z_{i,j} = \hat{e}_j^T(X_i-\hat{\mu})$ which gives us $$X_i = \hat{\mu} + \sum_{j=1}^{p \wedge N} z_{i,j}\hat{e}_j.$$ In practice, the process is as follows:

* Define $\tilde{X} := X - \hat{\mu} \in \mathbb{R}^{N \times p}$.
* Apply Singular Value Decomposition on $\tilde{X}$ such that $\tilde{X} = UDV^T$.
* Then, one gets $\hat{e}_j = V_{:,j}$ and $(z_{i,j})_{i,j} = UD$ are the scores.

```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
z <- seq(1,100)
perform_pca <- function(X,z){
  mu <- colMeans(X) # We substract the mean from X (see slides)
  X <- sweep(X,2,mu)
  
  SVD <- svd(X) # we compute X = UDV^T
  Scores <- SVD$u %*% diag(SVD$d) # computing (z_{n,j}) = UD 
  Loadings <- SVD$v # Eigenvectors

  FVE <- SVD$d^2/sum(SVD$d^2) # Fraction of variance explained

  lam <- sqrt(length(z)) # measure change
  
  # Plotting
  op <- par(mfrow=c(3,2),mar=rep(2,4))
  plot(z, X[1,]+mu,type="l", ylim=range(X+mu), main="Data and the mean")
  for(n in 1:dim(X)[1]) points(z, X[n,]+mu,type="l")
  points(z,mu,col=2,lwd=2,type="l")
  
  plot(Scores[1,]*sign(sum(Loadings[,1])), Scores[2,]*sign(sum(Loadings[,2])), main="1st vs 2nd PC scores",ylim=c(-10,2))
  
  plot(z,Loadings[,1]*sign(sum(Loadings[,1])),type="l", main=paste0("1st PC (",round(100*FVE[1])," % of var)"))
  # plot(z, X[1,]+mu,type="l", ylim=range(X+mu))
  # for(n in 1:dim(X)[1]) points(z, X[n,]+mu,type="l")
  # points(z,mu,col=2,lwd=2,type="l")
  # points(z,mu+3*SVD$d[1]/lam*SVD$v[,1],col=2,lwd=2,type="l",lty=2)
  # points(z,mu-3*SVD$d[1]/lam*SVD$v[,1],col=2,lwd=2,type="l",lty=2)
  
  plot(z,Loadings[,2]*sign(sum(Loadings[,2])),type="l", main=paste0("2nd PC (",round(100*FVE[2])," % of var)"))
  # plot(z, X[1,]+mu,type="l", ylim=range(X+mu))
  # for(n in 1:dim(X)[1]) points(z, X[n,]+mu,type="l")
  # points(z,mu,col=2,lwd=2,type="l")
  # points(z,mu+10*SVD$d[2]/lam*SVD$v[,2],col=2,lwd=2,type="l",lty=2)
  # points(z,mu-10*SVD$d[2]/lam*SVD$v[,2],col=2,lwd=2,type="l",lty=2)
  
  plot(z,Loadings[,3]*sign(sum(Loadings[,3])),type="l", main=paste0("3rd PC (",round(100*FVE[3])," % of var)"))
  # plot(z, X[1,]+mu,type="l", ylim=range(X+mu))
  # for(n in 1:dim(X)[1]) points(z, X[n,]+mu,type="l")
  # points(z,mu,col=2,lwd=2,type="l")
  # points(z,mu+30*SVD$d[3]/lam*SVD$v[,3],col=2,lwd=2,type="l",lty=2)
  # points(z,mu-30*SVD$d[3]/lam*SVD$v[,3],col=2,lwd=2,type="l",lty=2)
  
  plot(z,Loadings[,4]*sign(sum(Loadings[,4])),type="l", main=paste0("4th PC (",round(100*FVE[4])," % of var)"))
  # plot(z, X[1,]+mu,type="l", ylim=range(X+mu))
  # for(n in 1:dim(X)[1]) points(z, X[n,]+mu,type="l")
  # points(z,mu,col=2,lwd=2,type="l")
  # points(z,mu+30*SVD$d[4]/lam*SVD$v[,4],col=2,lwd=2,type="l",lty=2)
  # points(z,mu-30*SVD$d[4]/lam*SVD$v[,4],col=2,lwd=2,type="l",lty=2)
  return(c(Scores[1,]*sign(sum(Loadings[,1])), Scores[2,]*sign(sum(Loadings[,2]))))
}
out <- perform_pca(df_matrix,z)
```


```{r,fig.align='center', fig.show='hold',fig.dim=c(10,5)}
gdp_2019 <- c(
  455.032, 534.261, 67.038, 246.938, 355.770, 267.289, 2715.518, 3861.124, 
  209.853, 164.848, 385.034, 1854.763, 914.883, 403.180, 595.145, 238.359, 
  250.670, 54.874, 105.153, 1394.119, 530.155, 703.080, 153.609, 2827.110
)
df <- data.frame(
  pc1_scores = out[1:24],
  pc2_scores = out[25:48],
  gdp = gdp_2019
)

df$ names <- countries
df

ggplot(df, aes(pc1_scores, pc2_scores,label=countries)) +
  geom_point(aes(colour = gdp)) + geom_text(hjust=0, vjust=0,size=4)
```